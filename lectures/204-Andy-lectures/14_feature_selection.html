
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Lecture 14: Feature engineering and feature selection &#8212; CPSC 330 Applied Machine Learning 2024W1</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/extra.css?v=6df0ab2b" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lectures/204-Andy-lectures/14_feature_selection';</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/UBC-CS-logo.png" class="logo__image only-light" alt="CPSC 330 Applied Machine Learning 2024W1 - Home"/>
    <script>document.write(`<img src="../../_static/UBC-CS-logo.png" class="logo__image only-dark" alt="CPSC 330 Applied Machine Learning 2024W1 - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../README.html">
                    UBC CPSC 330: Applied Machine Learning (2024W2)
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Things you should know</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../syllabus.html">Syllabus</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../docs/README.html">CPSC 330 Documents</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../learning-objectives.html">Course Learning Objectives</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Lectures</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../notes/01_intro.html">Lecture 1: Course Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/02_terminology-decision-trees.html">Lecture 2: Terminology, Baselines, Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/03_ml-fundamentals.html">Lecture 3: Machine Learning Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/04_kNNs-SVM-RBF.html">Lecture 4: <span class="math notranslate nohighlight">\(k\)</span>-Nearest Neighbours and SVM RBFs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/05_preprocessing-pipelines.html">Lecture 5: Preprocessing and <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> pipelines</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/06_column-transformer-text-feats.html">Lecture 6: <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> <code class="docutils literal notranslate"><span class="pre">ColumnTransformer</span></code> and Text Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/07_linear-models.html">Lecture 7: Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/08_hyperparameter-optimization.html">Lecture 8: Hyperparameter Optimization and Optimization Bias</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/09_classification-metrics.html">Lecture 9: Classification metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/10_regression-metrics.html">Lecture 10: Regression metrics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/12_ensembles.html">Lecture 12: Ensembles</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/13_feat-importances.html">Lecture 13: Feature importances and model transparency</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/14_feature-engineering-selection.html">Lecture 14: Feature engineering and feature selection</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/15_K-Means.html">Lecture 15: K-Means Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/16_DBSCAN-hierarchical.html">Lecture 16: More Clustering</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/17_recommender-systems.html">Lecture 17: Recommender Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/18_natural-language-processing.html">Lecture 18: Introduction to natural language processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/19_intro_to_computer-vision.html">Lecture 19: Multi-class classification and introduction to computer vision</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/20_time-series.html">Lecture 20: Time series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/21_survival-analysis.html">Lecture 21: Survival analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/22_communication.html">Lecture 22: Communication</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/24_deployment-conclusion.html">Lecture 24: Deployment and conclusion</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/final-exam-review-guiding-question.html">Final exam preparation: guiding questions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/appendixA_feature-engineering-text-data.html">Appendix A: Demo of feature engineering for text data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/appendixB_multiclass-strategies.html">Appendix B: Multi-class, meta-strategies</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Attribution</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../LICENSE.html">LICENSE</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/UBC-CS/cpsc330-2024W2" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/lectures/204-Andy-lectures/14_feature_selection.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Lecture 14: Feature engineering and feature selection</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#announcements">Announcements</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">Learning outcomes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-engineering-motivation">Feature engineering: Motivation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iclicker-exercise-14-1">iClicker Exercise 14.1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#garbage-in-garbage-out">Garbage in, garbage out.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-feature-engineering">What is feature engineering?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-quotes-on-feature-engineering">Some quotes on feature engineering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#better-features-usually-help-more-than-a-better-model">Better features usually help more than a better model.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-best-features-may-be-dependent-on-the-model-you-use">The best features may be dependent on the model you use.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#domain-specific-transformations">Domain-specific transformations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#domain-specific-transformations-genomics">Domain-specific transformations (Genomics)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Domain-specific transformations (Genomics)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-interactions-and-feature-crosses">Feature interactions and feature crosses</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-crosses-for-one-hot-encoded-features">Feature crosses for one-hot encoded features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#demo-of-feature-engineering-with-numeric-features">Demo of feature engineering with numeric features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interim-summary">Interim summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-engineering">Feature engineering</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-introduction-and-motivation">Feature selection: Introduction and motivation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-feature-selection">What is feature selection?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-feature-selection">Why feature selection?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-carry-out-feature-selection">How do we carry out feature selection?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-models-recap-optional">Linear models recap (Optional)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression-l2-penalty-optional">Ridge regression (L2 penalty) (Optional)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression-l1-penalty-optional">Lasso regression (L1 penalty) (Optional)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-l1-and-l2-optional">Logistic regression (L1 and L2) (Optional)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional">(Optional)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l1-vs-l2-penalty-summary-optional">L1 vs L2 penalty summary (Optional)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-based-selection">Model-based selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iclicker-exercise-14-2">(iClicker) Exercise 14.2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recursive-feature-elimination-rfe">Recursive feature elimination (RFE)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rfe-algorithm">RFE algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-search-and-score">(Optional) Search and score</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-idea-of-search-and-score-methods">General idea of search and score methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-forward-or-backward-selection">(Optional) Forward or backward selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-ways-to-search">Other ways to search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iclicker-exercise-14-3">(iClicker) Exercise 14.3</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-problems-with-feature-selection">(Optional) Problems with feature selection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-is-relevance-clearly-defined">Example: Is “Relevance” clearly defined?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#is-relevance-clearly-defined">Is “Relevance” clearly defined?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Is “Relevance” clearly defined?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Is “Relevance” clearly defined?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Is “Relevance” clearly defined?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Is “Relevance” clearly defined?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Is “Relevance” clearly defined?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Is “Relevance” clearly defined?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warnings-about-feature-selection">Warnings about feature selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-advice-on-finding-relevant-features">General advice on finding relevant features</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#relevant-resources">Relevant resources</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">sys</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy.random</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">npr</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sb</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.compose</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">ColumnTransformer</span><span class="p">,</span>
    <span class="n">TransformedTargetRegressor</span><span class="p">,</span>
    <span class="n">make_column_transformer</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.dummy</span><span class="w"> </span><span class="kn">import</span> <span class="n">DummyClassifier</span><span class="p">,</span> <span class="n">DummyRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.impute</span><span class="w"> </span><span class="kn">import</span> <span class="n">SimpleImputer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">RidgeCV</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_scorer</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">r2_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">cross_validate</span><span class="p">,</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neighbors</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.pipeline</span><span class="w"> </span><span class="kn">import</span> <span class="n">Pipeline</span><span class="p">,</span> <span class="n">make_pipeline</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">OneHotEncoder</span><span class="p">,</span> <span class="n">OrdinalEncoder</span><span class="p">,</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SVC</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">mean_std_cross_val_scores</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns mean and std of cross validation</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    model :</span>
<span class="sd">        scikit-learn model</span>
<span class="sd">    X_train : numpy array or pandas DataFrame</span>
<span class="sd">        X in the training data</span>
<span class="sd">    y_train :</span>
<span class="sd">        y in the training data</span>

<span class="sd">    Returns</span>
<span class="sd">    ----------</span>
<span class="sd">        pandas Series with mean scores from cross_validation</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="n">mean_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">std_scores</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
    <span class="n">out_col</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mean_scores</span><span class="p">)):</span>
        <span class="n">out_col</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="sa">f</span><span class="s2">&quot;%0.3f (+/- %0.3f)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">mean_scores</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">std_scores</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">i</span><span class="p">])))</span>

    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">out_col</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">mean_scores</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="lecture-14-feature-engineering-and-feature-selection">
<h1>Lecture 14: Feature engineering and feature selection<a class="headerlink" href="#lecture-14-feature-engineering-and-feature-selection" title="Link to this heading">#</a></h1>
<p>UBC 2025</p>
<p>Instructor: Andrew Roth</p>
<section id="announcements">
<h2>Announcements<a class="headerlink" href="#announcements" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Final exam dates and registration posted. See Piazza.</p></li>
<li><p>MT1 marked. Viewing in CBTF bookings open. See Piazza.</p></li>
<li><p>HW5 is due March 10th (start now!)</p></li>
</ul>
</section>
<section id="learning-outcomes">
<h2>Learning outcomes<a class="headerlink" href="#learning-outcomes" title="Link to this heading">#</a></h2>
<p>From this lecture, students are expected to be able to:</p>
<ul class="simple">
<li><p>Explain what feature engineering is and the importance of feature engineering in building machine learning models.</p></li>
<li><p>Carry out preliminary feature engineering on numeric and text data.</p></li>
<li><p>Explain the general concept of feature selection.</p></li>
<li><p>Discuss and compare different feature selection methods at a high level.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>’s implementation of model-based selection and recursive feature elimination (<code class="docutils literal notranslate"><span class="pre">RFE</span></code>)</p></li>
</ul>
</section>
<section id="feature-engineering-motivation">
<h2>Feature engineering: Motivation<a class="headerlink" href="#feature-engineering-motivation" title="Link to this heading">#</a></h2>
<section id="iclicker-exercise-14-1">
<h3>iClicker Exercise 14.1<a class="headerlink" href="#iclicker-exercise-14-1" title="Link to this heading">#</a></h3>
<p><strong>iClicker cloud join link: https://join.iclicker.com/HTRZ</strong></p>
<p><strong>Select the most accurate option below.</strong></p>
<p>Suppose you are working on a machine learning project. If you have to prioritize one of the following in your project which of the following would it be?</p>
<ul class="simple">
<li><p>(A) The quality and size of the data</p></li>
<li><p>(B) Most recent deep neural network model</p></li>
<li><p>(C) Most recent optimization algorithm</p></li>
<li><p>(D) Domain expertise</p></li>
</ul>
<p><strong>Discussion question</strong></p>
<ul class="simple">
<li><p>Suppose we want to predict whether a flight will arrive on time or be delayed. We have a dataset with the following information about flights:</p>
<ul>
<li><p>Departure Time</p></li>
<li><p>Expected Duration of Flight (in minutes)</p></li>
</ul>
</li>
</ul>
<p>Upon analyzing the data, you notice a pattern: flights tend to be delayed more often during the evening rush hours. What feature could be valuable to add for this prediction task?
<br><br><br><br></p>
</section>
<section id="garbage-in-garbage-out">
<h3>Garbage in, garbage out.<a class="headerlink" href="#garbage-in-garbage-out" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Model building is interesting. But in your machine learning projects, you’ll be spending more than half of your time on data preparation, feature engineering, and transformations.</p></li>
<li><p>The <em>quality</em> of the data is important. Your model is only as good as your data.</p></li>
</ul>
</section>
<section id="what-is-feature-engineering">
<h3>What is feature engineering?<a class="headerlink" href="#what-is-feature-engineering" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Better features: more flexibility, higher score, we can get by with simple and more interpretable models.</p></li>
<li><p>If your features, i.e., representation is bad, whatever fancier model you build is not going to help.</p></li>
</ul>
<blockquote>
<b>Feature engineering</b> is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.<br> 
- Jason Brownlee    
</blockquote>    </section>
<section id="some-quotes-on-feature-engineering">
<h3>Some quotes on feature engineering<a class="headerlink" href="#some-quotes-on-feature-engineering" title="Link to this heading">#</a></h3>
<p>A quote by Pedro Domingos <a class="reference external" href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf">A Few Useful Things to Know About Machine Learning</a></p>
<blockquote>
... At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used. 
</blockquote>
<p>A quote by Andrew Ng, <a class="reference external" href="https://ai.stanford.edu/~ang/slides/DeepLearning-Mar2013.pptx">Machine Learning and AI via Brain simulations</a></p>
<blockquote>
Coming up with features is difficult, time-consuming, requires expert knowledge. "Applied machine learning" is basically feature engineering.
</blockquote></section>
<section id="better-features-usually-help-more-than-a-better-model">
<h3>Better features usually help more than a better model.<a class="headerlink" href="#better-features-usually-help-more-than-a-better-model" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Good features would ideally:</p>
<ul>
<li><p>Capture most important aspects of the problem</p></li>
<li><p>Allow learning with few examples</p></li>
<li><p>Generalize to new scenarios.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>There is a trade-off between simple and expressive features:</p>
<ul>
<li><p>With simple features overfitting risk is low, but scores might be low.</p></li>
<li><p>With complicated features scores can be high, but so is overfitting risk.</p></li>
</ul>
</li>
</ul>
<ul class="simple">
<li><p>The best features may be dependent on the model you use</p></li>
</ul>
</section>
<section id="the-best-features-may-be-dependent-on-the-model-you-use">
<h3>The best features may be dependent on the model you use.<a class="headerlink" href="#the-best-features-may-be-dependent-on-the-model-you-use" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Examples:</p>
<ul>
<li><p>For counting-based methods like decision trees separate relevant groups of variable values</p>
<ul>
<li><p>Discretization makes sense</p></li>
</ul>
</li>
<li><p>For distance-based methods like KNN, we want different class labels to be “far”.</p>
<ul>
<li><p>Standardization</p></li>
</ul>
</li>
<li><p>For regression-based methods like linear regression, we want targets to have a linear dependency on features.</p></li>
</ul>
</li>
</ul>
</section>
<section id="domain-specific-transformations">
<h3>Domain-specific transformations<a class="headerlink" href="#domain-specific-transformations" title="Link to this heading">#</a></h3>
<p>In some domains there are natural transformations to do:</p>
<ul class="simple">
<li><p>Spectrograms (sound data)</p></li>
<li><p>Wavelets (image data)</p></li>
<li><p>Convolutions</p></li>
<li><p>Read counts (genomics)</p></li>
</ul>
<p><img alt="" src="../../_images/spectogram.png" /></p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Spectrogram">Source</a></p>
</section>
<section id="domain-specific-transformations-genomics">
<h3>Domain-specific transformations (Genomics)<a class="headerlink" href="#domain-specific-transformations-genomics" title="Link to this heading">#</a></h3>
<p><img alt="" src="../../_images/genomics_snv_counts.png" /></p>
</section>
<section id="id1">
<h3>Domain-specific transformations (Genomics)<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://academic.oup.com/bioinformatics/article/28/7/907/209164">Unsupervised ML with simple features</a></p></li>
<li><p><a class="reference external" href="https://academic.oup.com/bioinformatics/article/28/2/167/197256">Supervised ML with complicated features</a></p></li>
</ul>
</section>
<section id="feature-interactions-and-feature-crosses">
<h3>Feature interactions and feature crosses<a class="headerlink" href="#feature-interactions-and-feature-crosses" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>A <strong>feature cross</strong> is a synthetic feature formed by multiplying or crossing two or more features.</p></li>
<li><p>Example:
Is the following dataset (XOR function) linearly separable?</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
    <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
    <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">]),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="s2">&quot;X2&quot;</span><span class="p">,</span> <span class="s2">&quot;target&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X1</th>
      <th>X2</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1</td>
      <td>-1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>-1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">sb</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;X2&quot;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;target&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Axes: xlabel=&#39;X1&#39;, ylabel=&#39;X2&#39;&gt;
</pre></div>
</div>
<img alt="../../_images/c9b5828aab7c1937f348a5397632c797e9d7283e1d84f377a0267df9efdbd793.png" src="../../_images/c9b5828aab7c1937f348a5397632c797e9d7283e1d84f377a0267df9efdbd793.png" />
</div>
</div>
<ul class="simple">
<li><p>For XOR like problems, if we create a feature cross <span class="math notranslate nohighlight">\(x1x2\)</span>, the data becomes linearly separable.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;X1X2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;X1&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;X2&quot;</span><span class="p">]</span>
<span class="n">df</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>X1</th>
      <th>X2</th>
      <th>target</th>
      <th>X1X2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-1</td>
      <td>-1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>-1</td>
      <td>0</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-1</td>
      <td>1</td>
      <td>0</td>
      <td>-1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">sb</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;X2&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;X1X2&quot;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;target&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Axes: xlabel=&#39;X2&#39;, ylabel=&#39;X1X2&#39;&gt;
</pre></div>
</div>
<img alt="../../_images/5934b1f059fac1d770bcf0dfee10b96f7f0c4cdf8718eba7a34e7e7100aab1ce.png" src="../../_images/5934b1f059fac1d770bcf0dfee10b96f7f0c4cdf8718eba7a34e7e7100aab1ce.png" />
</div>
</div>
<p>Let’s look at an example with more data points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">200</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">logical_xor</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">np</span><span class="o">.</span><span class="n">column_stack</span><span class="p">([</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">]),</span> <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="s2">&quot;X2&quot;</span><span class="p">,</span> <span class="s2">&quot;target&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">sb</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;X2&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;target&quot;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;target&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Axes: xlabel=&#39;X1&#39;, ylabel=&#39;X2&#39;&gt;
</pre></div>
</div>
<img alt="../../_images/2218b8de8d3ec74e87735b5d38b185aeabe9d31c447229c501cf457dd85617a9.png" src="../../_images/2218b8de8d3ec74e87735b5d38b185aeabe9d31c447229c501cf457dd85617a9.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">LogisticRegression</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.535
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">PolynomialFeatures</span>

<span class="n">pipe_xor</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">interaction_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">include_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="p">)</span>

<span class="n">pipe_xor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.995
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create an interactive 3D scatter plot using plotly</span>
<span class="c1"># import plotly.express as px</span>
<span class="c1"># df[&quot;X1_X2&quot;] = df[&quot;X1&quot;] * df[&quot;X2&quot;]</span>
<span class="c1"># fig = px.scatter_3d(df, x=&quot;X1&quot;, y=&quot;X2&quot;, z=&quot;X1_X2&quot;, color=&quot;target&quot;, color_continuous_scale=[&#39;blue&#39;, &#39;red&#39;])</span>
<span class="c1"># fig.show();</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;X1_X2&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;X1&quot;</span><span class="p">]</span> <span class="o">*</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;X2&quot;</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">sb</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s2">&quot;X1&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;X1_X2&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;target&quot;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s2">&quot;target&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">legend</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;Axes: xlabel=&#39;X1&#39;, ylabel=&#39;X1_X2&#39;&gt;
</pre></div>
</div>
<img alt="../../_images/ad96dde7d238629b63dd5f97ff6ee5bd3ba0f3e6eb69d1e15d8a8a00c87f8a49.png" src="../../_images/ad96dde7d238629b63dd5f97ff6ee5bd3ba0f3e6eb69d1e15d8a8a00c87f8a49.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">feature_names</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">pipe_xor</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;polynomialfeatures&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">pipe_xor</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;logisticregression&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">transpose</span><span class="p">(),</span>
    <span class="n">index</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Feature coefficient&quot;</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Feature coefficient</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>x0</th>
      <td>-0.028672</td>
    </tr>
    <tr>
      <th>x1</th>
      <td>0.129459</td>
    </tr>
    <tr>
      <th>x0 x1</th>
      <td>-5.086461</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The interaction feature has the biggest coefficient!</p>
</section>
<section id="feature-crosses-for-one-hot-encoded-features">
<h3>Feature crosses for one-hot encoded features<a class="headerlink" href="#feature-crosses-for-one-hot-encoded-features" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>You can think of feature crosses of one-hot-features as logical conjunctions</p></li>
<li><p>Suppose you want to predict whether you will find parking or not based on two features:</p>
<ul>
<li><p>area (possible categories: UBC campus and Rogers Arena)</p></li>
<li><p>time of the day (possible categories: 9am and 7pm)</p></li>
</ul>
</li>
<li><p>A feature cross in this case would create four new features:</p>
<ul>
<li><p>UBC campus and 9am</p></li>
<li><p>UBC campus and 7pm</p></li>
<li><p>Rogers Arena and 9am</p></li>
<li><p>Rogers Arena and 7pm.</p></li>
</ul>
</li>
<li><p>The features UBC campus and 9am on their own are not that informative but the newly created feature UBC campus and 9am or Rogers Arena and 7pm would be quite informative.</p></li>
</ul>
<ul class="simple">
<li><p>Coming up with the right combination of features requires some domain knowledge or careful examination of the data.</p></li>
<li><p>There is no easy way to support feature crosses in sklearn.</p></li>
</ul>
</section>
<section id="demo-of-feature-engineering-with-numeric-features">
<h3>Demo of feature engineering with numeric features<a class="headerlink" href="#demo-of-feature-engineering-with-numeric-features" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Remember the <a class="reference external" href="https://www.kaggle.com/datasets/camnugent/california-housing-prices">California housing dataset</a> we used earlier in the course?</p></li>
<li><p>The prediction task is predicting <code class="docutils literal notranslate"><span class="pre">median_house_value</span></code> for a given community.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">housing_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;../data/california_housing.csv&quot;</span><span class="p">)</span>
<span class="n">housing_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>median_house_value</th>
      <th>ocean_proximity</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>-122.23</td>
      <td>37.88</td>
      <td>41.0</td>
      <td>880.0</td>
      <td>129.0</td>
      <td>322.0</td>
      <td>126.0</td>
      <td>8.3252</td>
      <td>452600.0</td>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-122.22</td>
      <td>37.86</td>
      <td>21.0</td>
      <td>7099.0</td>
      <td>1106.0</td>
      <td>2401.0</td>
      <td>1138.0</td>
      <td>8.3014</td>
      <td>358500.0</td>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>2</th>
      <td>-122.24</td>
      <td>37.85</td>
      <td>52.0</td>
      <td>1467.0</td>
      <td>190.0</td>
      <td>496.0</td>
      <td>177.0</td>
      <td>7.2574</td>
      <td>352100.0</td>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-122.25</td>
      <td>37.85</td>
      <td>52.0</td>
      <td>1274.0</td>
      <td>235.0</td>
      <td>558.0</td>
      <td>219.0</td>
      <td>5.6431</td>
      <td>341300.0</td>
      <td>NEAR BAY</td>
    </tr>
    <tr>
      <th>4</th>
      <td>-122.25</td>
      <td>37.85</td>
      <td>52.0</td>
      <td>1627.0</td>
      <td>280.0</td>
      <td>565.0</td>
      <td>259.0</td>
      <td>3.8462</td>
      <td>342200.0</td>
      <td>NEAR BAY</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">housing_df</span><span class="o">.</span><span class="n">info</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 20640 entries, 0 to 20639
Data columns (total 10 columns):
 #   Column              Non-Null Count  Dtype  
---  ------              --------------  -----  
 0   longitude           20640 non-null  float64
 1   latitude            20640 non-null  float64
 2   housing_median_age  20640 non-null  float64
 3   total_rooms         20640 non-null  float64
 4   total_bedrooms      20433 non-null  float64
 5   population          20640 non-null  float64
 6   households          20640 non-null  float64
 7   median_income       20640 non-null  float64
 8   median_house_value  20640 non-null  float64
 9   ocean_proximity     20640 non-null  object 
dtypes: float64(9), object(1)
memory usage: 1.6+ MB
</pre></div>
</div>
</div>
</div>
<p>Suppose we decide to train <code class="docutils literal notranslate"><span class="pre">ridge</span></code> model on this dataset.</p>
<ul class="simple">
<li><p>What would happen if you train a model without applying any transformation on the categorical features ocean_proximity?</p>
<ul>
<li><p>Error!! A linear model requires all features in a numeric form.</p></li>
</ul>
</li>
<li><p>What would happen if we apply OHE on <code class="docutils literal notranslate"><span class="pre">ocean_proximity</span></code> but we do not scale the numeric features?</p>
<ul>
<li><p>No syntax error. But the model results are likely to be poor.</p></li>
</ul>
</li>
<li><p>Do we need to apply any other transformations on this data?</p></li>
</ul>
<p>In this section, we will look into some common ways to do feature engineering for numeric features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">housing_df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We have total rooms and the number of households in the neighbourhood. How about creating rooms_per_household feature using this information?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
    <span class="n">rooms_per_household</span><span class="o">=</span><span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;total_rooms&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;households&quot;</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">test_df</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">assign</span><span class="p">(</span>
    <span class="n">rooms_per_household</span><span class="o">=</span><span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;total_rooms&quot;</span><span class="p">]</span> <span class="o">/</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;households&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>longitude</th>
      <th>latitude</th>
      <th>housing_median_age</th>
      <th>total_rooms</th>
      <th>total_bedrooms</th>
      <th>population</th>
      <th>households</th>
      <th>median_income</th>
      <th>median_house_value</th>
      <th>ocean_proximity</th>
      <th>rooms_per_household</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>9950</th>
      <td>-122.33</td>
      <td>38.38</td>
      <td>28.0</td>
      <td>1020.0</td>
      <td>169.0</td>
      <td>504.0</td>
      <td>164.0</td>
      <td>4.5694</td>
      <td>287500.0</td>
      <td>INLAND</td>
      <td>6.219512</td>
    </tr>
    <tr>
      <th>3547</th>
      <td>-118.60</td>
      <td>34.26</td>
      <td>18.0</td>
      <td>6154.0</td>
      <td>1070.0</td>
      <td>3010.0</td>
      <td>1034.0</td>
      <td>5.6392</td>
      <td>271500.0</td>
      <td>&lt;1H OCEAN</td>
      <td>5.951644</td>
    </tr>
    <tr>
      <th>4448</th>
      <td>-118.21</td>
      <td>34.07</td>
      <td>47.0</td>
      <td>1346.0</td>
      <td>383.0</td>
      <td>1452.0</td>
      <td>371.0</td>
      <td>1.7292</td>
      <td>191700.0</td>
      <td>&lt;1H OCEAN</td>
      <td>3.628032</td>
    </tr>
    <tr>
      <th>6984</th>
      <td>-118.02</td>
      <td>33.96</td>
      <td>36.0</td>
      <td>2071.0</td>
      <td>398.0</td>
      <td>988.0</td>
      <td>404.0</td>
      <td>4.6226</td>
      <td>219700.0</td>
      <td>&lt;1H OCEAN</td>
      <td>5.126238</td>
    </tr>
    <tr>
      <th>4432</th>
      <td>-118.20</td>
      <td>34.08</td>
      <td>49.0</td>
      <td>1320.0</td>
      <td>309.0</td>
      <td>1405.0</td>
      <td>328.0</td>
      <td>2.4375</td>
      <td>114000.0</td>
      <td>&lt;1H OCEAN</td>
      <td>4.024390</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let’s start simple. Imagine that we only three features: <code class="docutils literal notranslate"><span class="pre">longitude</span></code>, <code class="docutils literal notranslate"><span class="pre">latitude</span></code>, and our newly created <code class="docutils literal notranslate"><span class="pre">rooms_per_household</span></code> feature.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train_housing</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[[</span><span class="s2">&quot;latitude&quot;</span><span class="p">,</span> <span class="s2">&quot;longitude&quot;</span><span class="p">,</span> <span class="s2">&quot;rooms_per_household&quot;</span><span class="p">]]</span>
<span class="n">y_train_housing</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;median_house_value&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.compose</span><span class="w"> </span><span class="kn">import</span> <span class="n">make_column_transformer</span>

<span class="n">numeric_feats</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;latitude&quot;</span><span class="p">,</span> <span class="s2">&quot;longitude&quot;</span><span class="p">,</span> <span class="s2">&quot;rooms_per_household&quot;</span><span class="p">]</span>

<span class="n">preprocessor1</span> <span class="o">=</span> <span class="n">make_column_transformer</span><span class="p">(</span>
    <span class="p">(</span><span class="n">make_pipeline</span><span class="p">(</span><span class="n">SimpleImputer</span><span class="p">(),</span> <span class="n">StandardScaler</span><span class="p">()),</span> <span class="n">numeric_feats</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_1</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">preprocessor1</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">())</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">cross_validate</span><span class="p">(</span><span class="n">lr_1</span><span class="p">,</span> <span class="n">X_train_housing</span><span class="p">,</span> <span class="n">y_train_housing</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.006068</td>
      <td>0.001843</td>
      <td>0.280028</td>
      <td>0.311769</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.003365</td>
      <td>0.001635</td>
      <td>0.325319</td>
      <td>0.300464</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.003764</td>
      <td>0.001509</td>
      <td>0.317277</td>
      <td>0.301952</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.003215</td>
      <td>0.001604</td>
      <td>0.316798</td>
      <td>0.303004</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.003489</td>
      <td>0.001239</td>
      <td>0.260258</td>
      <td>0.314840</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>The scores are not great.</p></li>
<li><p>Let’s look at the distribution of the longitude and latitude features.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">80</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;longitude&quot;</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Distribution of latitude feature&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/94295a49182e2ab3ec008f7ac4c47f323ddbebb9cc10d828e35f24a895907a24.png" src="../../_images/94295a49182e2ab3ec008f7ac4c47f323ddbebb9cc10d828e35f24a895907a24.png" />
</div>
</div>
<ul class="simple">
<li><p>Suppose you are planning to build a linear model for housing price prediction.</p></li>
<li><p>If we think longitude is a good feature for prediction, does it makes sense to use the floating point representation of this feature that’s given to us?</p></li>
<li><p>Remember that linear models can capture only linear relationships.</p></li>
</ul>
<ul class="simple">
<li><p>How about discretizing latitude and longitude features and putting them into buckets?</p></li>
<li><p>This process of transforming numeric features into categorical features is called bucketing or binning.</p></li>
<li><p>In <code class="docutils literal notranslate"><span class="pre">sklearn</span></code> you can do this using <code class="docutils literal notranslate"><span class="pre">KBinsDiscretizer</span></code> transformer.</p></li>
<li><p>Let’s examine whether we get better results with binning.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">KBinsDiscretizer</span>

<span class="n">discretization_feats</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;latitude&quot;</span><span class="p">,</span> <span class="s2">&quot;longitude&quot;</span><span class="p">]</span>
<span class="n">numeric_feats</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;rooms_per_household&quot;</span><span class="p">]</span>

<span class="n">preprocessor2</span> <span class="o">=</span> <span class="n">make_column_transformer</span><span class="p">(</span>
    <span class="p">(</span><span class="n">KBinsDiscretizer</span><span class="p">(</span><span class="n">n_bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">encode</span><span class="o">=</span><span class="s2">&quot;onehot&quot;</span><span class="p">),</span> <span class="n">discretization_feats</span><span class="p">),</span>
    <span class="p">(</span><span class="n">make_pipeline</span><span class="p">(</span><span class="n">SimpleImputer</span><span class="p">(),</span> <span class="n">StandardScaler</span><span class="p">()),</span> <span class="n">numeric_feats</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_2</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">preprocessor2</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">())</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">cross_validate</span><span class="p">(</span><span class="n">lr_2</span><span class="p">,</span> <span class="n">X_train_housing</span><span class="p">,</span> <span class="n">y_train_housing</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.013504</td>
      <td>0.004102</td>
      <td>0.441445</td>
      <td>0.456419</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.013672</td>
      <td>0.004972</td>
      <td>0.469571</td>
      <td>0.446216</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.014227</td>
      <td>0.003736</td>
      <td>0.479132</td>
      <td>0.446869</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.013462</td>
      <td>0.003769</td>
      <td>0.450822</td>
      <td>0.453367</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.015541</td>
      <td>0.004175</td>
      <td>0.388169</td>
      <td>0.467628</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>The results are better with binned features. Let’s examine the binned features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">preprocessor2</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train_housing</span><span class="p">)</span><span class="o">.</span><span class="n">todense</span><span class="p">(),</span>
    <span class="n">columns</span><span class="o">=</span><span class="n">preprocessor2</span><span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>kbinsdiscretizer__latitude_0.0</th>
      <th>kbinsdiscretizer__latitude_1.0</th>
      <th>kbinsdiscretizer__latitude_2.0</th>
      <th>kbinsdiscretizer__latitude_3.0</th>
      <th>kbinsdiscretizer__latitude_4.0</th>
      <th>kbinsdiscretizer__latitude_5.0</th>
      <th>kbinsdiscretizer__latitude_6.0</th>
      <th>kbinsdiscretizer__latitude_7.0</th>
      <th>kbinsdiscretizer__latitude_8.0</th>
      <th>kbinsdiscretizer__latitude_9.0</th>
      <th>...</th>
      <th>kbinsdiscretizer__longitude_11.0</th>
      <th>kbinsdiscretizer__longitude_12.0</th>
      <th>kbinsdiscretizer__longitude_13.0</th>
      <th>kbinsdiscretizer__longitude_14.0</th>
      <th>kbinsdiscretizer__longitude_15.0</th>
      <th>kbinsdiscretizer__longitude_16.0</th>
      <th>kbinsdiscretizer__longitude_17.0</th>
      <th>kbinsdiscretizer__longitude_18.0</th>
      <th>kbinsdiscretizer__longitude_19.0</th>
      <th>pipeline__rooms_per_household</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.316164</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.209903</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-0.711852</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-0.117528</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-0.554621</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>16507</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.064307</td>
    </tr>
    <tr>
      <th>16508</th>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.235706</td>
    </tr>
    <tr>
      <th>16509</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.212581</td>
    </tr>
    <tr>
      <th>16510</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-0.271037</td>
    </tr>
    <tr>
      <th>16511</th>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.027321</td>
    </tr>
  </tbody>
</table>
<p>16512 rows × 41 columns</p>
</div></div></div>
</div>
<p>How about discretizing all three features?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">KBinsDiscretizer</span>

<span class="n">discretization_feats</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;latitude&quot;</span><span class="p">,</span> <span class="s2">&quot;longitude&quot;</span><span class="p">,</span> <span class="s2">&quot;rooms_per_household&quot;</span><span class="p">]</span>

<span class="n">preprocessor3</span> <span class="o">=</span> <span class="n">make_column_transformer</span><span class="p">(</span>
    <span class="p">(</span><span class="n">KBinsDiscretizer</span><span class="p">(</span><span class="n">n_bins</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">encode</span><span class="o">=</span><span class="s2">&quot;onehot&quot;</span><span class="p">),</span> <span class="n">discretization_feats</span><span class="p">),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_3</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">preprocessor3</span><span class="p">,</span> <span class="n">Ridge</span><span class="p">())</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">cross_validate</span><span class="p">(</span><span class="n">lr_3</span><span class="p">,</span> <span class="n">X_train_housing</span><span class="p">,</span> <span class="n">y_train_housing</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>fit_time</th>
      <th>score_time</th>
      <th>test_score</th>
      <th>train_score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.026149</td>
      <td>0.004086</td>
      <td>0.590618</td>
      <td>0.571969</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.015383</td>
      <td>0.003433</td>
      <td>0.575907</td>
      <td>0.570473</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.012912</td>
      <td>0.002968</td>
      <td>0.579091</td>
      <td>0.573542</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.012584</td>
      <td>0.003278</td>
      <td>0.571500</td>
      <td>0.574260</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.022140</td>
      <td>0.005023</td>
      <td>0.541488</td>
      <td>0.581687</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>The results have improved further!!</p></li>
<li><p>Let’s examine the coefficients</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_housing</span><span class="p">,</span> <span class="n">y_train_housing</span><span class="p">)</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">lr_3</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;columntransformer&quot;</span><span class="p">]</span>
    <span class="o">.</span><span class="n">named_transformers_</span><span class="p">[</span><span class="s2">&quot;kbinsdiscretizer&quot;</span><span class="p">]</span>
    <span class="o">.</span><span class="n">get_feature_names_out</span><span class="p">()</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coefs_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">lr_3</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;ridge&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="o">.</span><span class="n">transpose</span><span class="p">(),</span>
    <span class="n">index</span><span class="o">=</span><span class="n">feature_names</span><span class="p">,</span>
    <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;coefficient&quot;</span><span class="p">],</span>
<span class="p">)</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="s2">&quot;coefficient&quot;</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">coefs_df</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>coefficient</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>longitude_1.0</th>
      <td>211343.036136</td>
    </tr>
    <tr>
      <th>latitude_1.0</th>
      <td>205059.296601</td>
    </tr>
    <tr>
      <th>latitude_0.0</th>
      <td>201862.534342</td>
    </tr>
    <tr>
      <th>longitude_0.0</th>
      <td>190319.721818</td>
    </tr>
    <tr>
      <th>longitude_2.0</th>
      <td>160282.191204</td>
    </tr>
    <tr>
      <th>longitude_3.0</th>
      <td>157234.920305</td>
    </tr>
    <tr>
      <th>latitude_2.0</th>
      <td>154105.963689</td>
    </tr>
    <tr>
      <th>rooms_per_household_19.0</th>
      <td>138503.477291</td>
    </tr>
    <tr>
      <th>latitude_8.0</th>
      <td>135299.516394</td>
    </tr>
    <tr>
      <th>longitude_4.0</th>
      <td>132292.924485</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Let’s focus on the rooms_per_household bins</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">room_rows</span> <span class="o">=</span> <span class="p">[</span><span class="n">x</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;rooms&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">coefs_df</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
<span class="n">coefs_df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">room_rows</span><span class="p">]</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s2">&quot;coefficient&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>coefficient</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>rooms_per_household_1.0</th>
      <td>-51143.091625</td>
    </tr>
    <tr>
      <th>rooms_per_household_0.0</th>
      <td>-50884.444297</td>
    </tr>
    <tr>
      <th>rooms_per_household_2.0</th>
      <td>-47606.596151</td>
    </tr>
    <tr>
      <th>rooms_per_household_5.0</th>
      <td>-43445.134061</td>
    </tr>
    <tr>
      <th>rooms_per_household_3.0</th>
      <td>-42060.071975</td>
    </tr>
    <tr>
      <th>rooms_per_household_4.0</th>
      <td>-40689.197649</td>
    </tr>
    <tr>
      <th>rooms_per_household_6.0</th>
      <td>-32734.570739</td>
    </tr>
    <tr>
      <th>rooms_per_household_7.0</th>
      <td>-30573.540359</td>
    </tr>
    <tr>
      <th>rooms_per_household_8.0</th>
      <td>-26919.381190</td>
    </tr>
    <tr>
      <th>rooms_per_household_9.0</th>
      <td>-19591.810098</td>
    </tr>
    <tr>
      <th>rooms_per_household_10.0</th>
      <td>-16630.535622</td>
    </tr>
    <tr>
      <th>rooms_per_household_11.0</th>
      <td>-12178.614567</td>
    </tr>
    <tr>
      <th>rooms_per_household_12.0</th>
      <td>1858.970683</td>
    </tr>
    <tr>
      <th>rooms_per_household_13.0</th>
      <td>9351.210272</td>
    </tr>
    <tr>
      <th>rooms_per_household_14.0</th>
      <td>16460.273962</td>
    </tr>
    <tr>
      <th>rooms_per_household_15.0</th>
      <td>31454.877046</td>
    </tr>
    <tr>
      <th>rooms_per_household_16.0</th>
      <td>44311.362553</td>
    </tr>
    <tr>
      <th>rooms_per_household_17.0</th>
      <td>70472.564483</td>
    </tr>
    <tr>
      <th>rooms_per_household_18.0</th>
      <td>102044.252042</td>
    </tr>
    <tr>
      <th>rooms_per_household_19.0</th>
      <td>138503.477291</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<ul class="simple">
<li><p>Does it make sense to take feature crosses in this context?</p></li>
<li><p>What information would they encode?</p></li>
</ul>
</section>
<section id="interim-summary">
<h3>Interim summary<a class="headerlink" href="#interim-summary" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Feature engineering is finding the useful representation of the data that can help us effectively solve our problem.</p></li>
</ul>
<p>Check out <a class="reference internal" href="#appendixA_feature-engineering-text-data.ipynb"><span class="xref myst">Appendix A</span></a> for a demo of feature engineering on text data.</p>
<ul class="simple">
<li><p>In the context of text data, if we want to go beyond bag-of-words and incorporate human knowledge in models, we carry out feature engineering.</p></li>
<li><p>Some common features include:</p>
<ul>
<li><p>ngram features</p></li>
<li><p>part-of-speech features</p></li>
<li><p>named entity features</p></li>
<li><p>emoticons in text</p></li>
</ul>
</li>
<li><p>These are usually extracted from pre-trained models using libraries such as <code class="docutils literal notranslate"><span class="pre">spaCy</span></code>.</p></li>
<li><p>Now a lot of this has moved to deep learning.</p></li>
<li><p>But many industries still rely on manual feature engineering.</p></li>
</ul>
</section>
<section id="feature-engineering">
<h3>Feature engineering<a class="headerlink" href="#feature-engineering" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The best features are application-dependent.</p></li>
<li><p>It’s hard to give general advice. But here are some guidelines.</p>
<ul>
<li><p>Ask the domain experts.</p></li>
<li><p>Go through academic papers in the discipline.</p></li>
<li><p>Often have idea of right discretization/standardization/transformation.</p></li>
<li><p>If no domain expert, cross-validation will help.</p></li>
</ul>
</li>
<li><p>If you have lots of data, use deep learning methods.</p></li>
</ul>
<blockquote>
    The algorithms we used are very standard for Kagglers ... We spent most of our efforts in feature engineering... <br>
- Xavier Conort, on winning the Flight Quest challenge on Kaggle    
</blockquote>    </section>
</section>
<section id="feature-selection-introduction-and-motivation">
<h2>Feature selection: Introduction and motivation<a class="headerlink" href="#feature-selection-introduction-and-motivation" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>With so many ways to add new features, we can increase dimensionality of the data.</p></li>
<li><p>More features means more complex models, which means increasing the chance of overfitting.</p></li>
</ul>
<section id="what-is-feature-selection">
<h3>What is feature selection?<a class="headerlink" href="#what-is-feature-selection" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Find the features	(columns) <span class="math notranslate nohighlight">\(X\)</span> that are important for predicting	<span class="math notranslate nohighlight">\(y\)</span>, and remove the features that aren’t.</p></li>
<li><p>Given <span class="math notranslate nohighlight">\(X = \begin{bmatrix}x_1 &amp; x_2 &amp; \dots &amp; x_n\\  \\  \\  \end{bmatrix}\)</span> and <span class="math notranslate nohighlight">\(y = \begin{bmatrix}\\  \\  \\  \end{bmatrix}\)</span>, find the columns <span class="math notranslate nohighlight">\(1 \leq j \leq n\)</span> in <span class="math notranslate nohighlight">\(X\)</span> that are important for predicting <span class="math notranslate nohighlight">\(y\)</span>.</p></li>
</ul>
</section>
<section id="why-feature-selection">
<h3>Why feature selection?<a class="headerlink" href="#why-feature-selection" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Interpretability: Models are more interpretable with fewer features. If you get the same performance with 10 features instead of 500 features, why not use the model with smaller number of features?</p></li>
<li><p>Computation: Models fit/predict faster with fewer columns.</p></li>
<li><p>Data collection: What type of new data should I collect? It may be cheaper to collect fewer columns.</p></li>
<li><p>Fundamental tradeoff: Can I reduce overfitting by removing useless features?</p></li>
</ul>
<p>Feature selection can often result in better performing (less overfit), easier to understand, and faster model.</p>
</section>
<section id="how-do-we-carry-out-feature-selection">
<h3>How do we carry out feature selection?<a class="headerlink" href="#how-do-we-carry-out-feature-selection" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>There are a number of ways.</p></li>
<li><p>You could use domain knowledge to discard features.</p></li>
<li><p>We are briefly going to look at some automatic feature selection methods from <code class="docutils literal notranslate"><span class="pre">sklearn</span></code>:</p>
<ul>
<li><p>Model-based selection</p></li>
<li><p>Recursive feature elimination</p></li>
<li><p>Forward/backward selection</p></li>
</ul>
</li>
<li><p>Very related to looking at feature importances.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_breast_cancer</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">(</span><span class="n">as_frame</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="s2">&quot;frame&quot;</span><span class="p">]</span>
<span class="n">train_df</span><span class="p">,</span> <span class="n">test_df</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">df</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span>
<span class="p">)</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>mean radius</th>
      <th>mean texture</th>
      <th>mean perimeter</th>
      <th>mean area</th>
      <th>mean smoothness</th>
      <th>mean compactness</th>
      <th>mean concavity</th>
      <th>mean concave points</th>
      <th>mean symmetry</th>
      <th>mean fractal dimension</th>
      <th>...</th>
      <th>worst texture</th>
      <th>worst perimeter</th>
      <th>worst area</th>
      <th>worst smoothness</th>
      <th>worst compactness</th>
      <th>worst concavity</th>
      <th>worst concave points</th>
      <th>worst symmetry</th>
      <th>worst fractal dimension</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>395</th>
      <td>14.06</td>
      <td>17.18</td>
      <td>89.75</td>
      <td>609.1</td>
      <td>0.08045</td>
      <td>0.05361</td>
      <td>0.02681</td>
      <td>0.03251</td>
      <td>0.1641</td>
      <td>0.05764</td>
      <td>...</td>
      <td>25.34</td>
      <td>96.42</td>
      <td>684.5</td>
      <td>0.1066</td>
      <td>0.1231</td>
      <td>0.0846</td>
      <td>0.07911</td>
      <td>0.2523</td>
      <td>0.06609</td>
      <td>1</td>
    </tr>
    <tr>
      <th>393</th>
      <td>21.61</td>
      <td>22.28</td>
      <td>144.40</td>
      <td>1407.0</td>
      <td>0.11670</td>
      <td>0.20870</td>
      <td>0.28100</td>
      <td>0.15620</td>
      <td>0.2162</td>
      <td>0.06606</td>
      <td>...</td>
      <td>28.74</td>
      <td>172.00</td>
      <td>2081.0</td>
      <td>0.1502</td>
      <td>0.5717</td>
      <td>0.7053</td>
      <td>0.24220</td>
      <td>0.3828</td>
      <td>0.10070</td>
      <td>0</td>
    </tr>
    <tr>
      <th>381</th>
      <td>11.04</td>
      <td>14.93</td>
      <td>70.67</td>
      <td>372.7</td>
      <td>0.07987</td>
      <td>0.07079</td>
      <td>0.03546</td>
      <td>0.02074</td>
      <td>0.2003</td>
      <td>0.06246</td>
      <td>...</td>
      <td>20.83</td>
      <td>79.73</td>
      <td>447.1</td>
      <td>0.1095</td>
      <td>0.1982</td>
      <td>0.1553</td>
      <td>0.06754</td>
      <td>0.3202</td>
      <td>0.07287</td>
      <td>1</td>
    </tr>
    <tr>
      <th>198</th>
      <td>19.18</td>
      <td>22.49</td>
      <td>127.50</td>
      <td>1148.0</td>
      <td>0.08523</td>
      <td>0.14280</td>
      <td>0.11140</td>
      <td>0.06772</td>
      <td>0.1767</td>
      <td>0.05529</td>
      <td>...</td>
      <td>32.06</td>
      <td>166.40</td>
      <td>1688.0</td>
      <td>0.1322</td>
      <td>0.5601</td>
      <td>0.3865</td>
      <td>0.17080</td>
      <td>0.3193</td>
      <td>0.09221</td>
      <td>0</td>
    </tr>
    <tr>
      <th>145</th>
      <td>11.90</td>
      <td>14.65</td>
      <td>78.11</td>
      <td>432.8</td>
      <td>0.11520</td>
      <td>0.12960</td>
      <td>0.03710</td>
      <td>0.03003</td>
      <td>0.1995</td>
      <td>0.07839</td>
      <td>...</td>
      <td>16.51</td>
      <td>86.26</td>
      <td>509.6</td>
      <td>0.1424</td>
      <td>0.2517</td>
      <td>0.0942</td>
      <td>0.06042</td>
      <td>0.2727</td>
      <td>0.10360</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 31 columns</p>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">])</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>

<span class="n">X_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">])</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="n">test_df</span><span class="p">[</span><span class="s2">&quot;target&quot;</span><span class="p">]</span>

<span class="n">X_train</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(284, 30)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_l2_pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">))</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cross_validate</span><span class="p">(</span><span class="n">lr_l2_pipe</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fit_time       0.002742
score_time     0.000868
test_score     0.968233
train_score    0.987681
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>How could you use the coefficients for feature selection?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_l2_pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">lr_l2_pipe</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;logisticregression&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mean radius               -0.226111
mean texture              -0.549475
mean perimeter            -0.221070
mean area                 -0.292530
mean smoothness           -0.068116
mean compactness           0.197617
mean concavity            -0.648555
mean concave points       -0.851493
mean symmetry             -0.444497
mean fractal dimension     0.098324
radius error              -1.317334
texture error             -0.026222
perimeter error           -0.862855
area error                -0.749326
smoothness error           0.327195
compactness error          0.835563
concavity error            0.018267
concave points error      -0.308506
symmetry error             0.135853
fractal dimension error    0.876668
worst radius              -0.799291
worst texture             -0.682827
worst perimeter           -0.625307
worst area                -0.679874
worst smoothness          -0.386772
worst compactness         -0.047397
worst concavity           -0.858240
worst concave points      -1.092499
worst symmetry            -0.356959
worst fractal dimension   -0.445707
dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_l2_pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">lr_l2_pipe</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;logisticregression&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>concavity error            0.018267
texture error              0.026222
worst compactness          0.047397
mean smoothness            0.068116
mean fractal dimension     0.098324
symmetry error             0.135853
mean compactness           0.197617
mean perimeter             0.221070
mean radius                0.226111
mean area                  0.292530
concave points error       0.308506
smoothness error           0.327195
worst symmetry             0.356959
worst smoothness           0.386772
mean symmetry              0.444497
worst fractal dimension    0.445707
mean texture               0.549475
worst perimeter            0.625307
mean concavity             0.648555
worst area                 0.679874
worst texture              0.682827
area error                 0.749326
worst radius               0.799291
compactness error          0.835563
mean concave points        0.851493
worst concavity            0.858240
perimeter error            0.862855
fractal dimension error    0.876668
worst concave points       1.092499
radius error               1.317334
dtype: float64
</pre></div>
</div>
</div>
</div>
</section>
<section id="linear-models-recap-optional">
<h3>Linear models recap (Optional)<a class="headerlink" href="#linear-models-recap-optional" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Recall that in linear regression our predictions are given by
$<span class="math notranslate nohighlight">\(\hat{y_i} = \sum_j w_j x_{i j} + b\)</span>$</p></li>
<li><p>To estimate the coefficients <span class="math notranslate nohighlight">\(w_j\)</span>, linear regression tries to minimize the following equation
$<span class="math notranslate nohighlight">\( \sum_i |\hat{y_i} - y_i|^2\)</span>$
that is the squared error between predicted and observed values.</p></li>
</ul>
</section>
<section id="ridge-regression-l2-penalty-optional">
<h3>Ridge regression (L2 penalty) (Optional)<a class="headerlink" href="#ridge-regression-l2-penalty-optional" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In this course we have said to use Ridge instead of LinearRegression to avoid overfitting the coefficients</p></li>
<li><p>In Ridge regression we still predict values using <span class="math notranslate nohighlight">\(\hat{y_i} = \sum_j w_j x_{i j} + b\)</span></p></li>
<li><p>But to estimate the coefficients we <strong>minimize</strong> a different equation
<br>
$<span class="math notranslate nohighlight">\( \sum_i |\hat{y_i} - y_i|^2 + \alpha \sum_j |w_j|^2 \)</span>$</p></li>
<li><p>The term <span class="math notranslate nohighlight">\(\sum_j |w_j|^2\)</span> is a penalty which discourages large coefficients.</p></li>
<li><p>The strength of the penalty is controlled by <span class="math notranslate nohighlight">\(\alpha\)</span> which we have seen can be tuned using cross-validation.</p></li>
</ul>
</section>
<section id="lasso-regression-l1-penalty-optional">
<h3>Lasso regression (L1 penalty) (Optional)<a class="headerlink" href="#lasso-regression-l1-penalty-optional" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>We can consider other penalties instead of <span class="math notranslate nohighlight">\(\sum_j |w_j|^2\)</span>.</p>
<ul>
<li><p>But we still predict values using <span class="math notranslate nohighlight">\(\hat{y_i} = \sum_j w_j x_{i j} + b\)</span></p></li>
</ul>
</li>
<li><p>A very common choice of penalty to use the absolute value <span class="math notranslate nohighlight">\(\sum_j |w_j|\)</span></p>
<ul>
<li><p>This called the L1 norm because the absolute value is to the power 1</p></li>
</ul>
</li>
<li><p>To estimate the coefficients we <strong>minimize</strong>
<br>
$<span class="math notranslate nohighlight">\( \sum_i |\hat{y_i} - y_i|^2 + \alpha \sum_j |w_j| \)</span>$</p></li>
<li><p>This is callled Lasso regression and is implemented in the <code class="docutils literal notranslate"><span class="pre">sklearn.linear_model.Lasso</span></code> class</p></li>
<li><p>An interesting feature of using the L1 penalty is that it will often set coefficients to zero!</p>
<ul>
<li><p>This can be considered a form of feature selection.</p></li>
</ul>
</li>
</ul>
</section>
<section id="logistic-regression-l1-and-l2-optional">
<h3>Logistic regression (L1 and L2) (Optional)<a class="headerlink" href="#logistic-regression-l1-and-l2-optional" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>We have focused on linear regression so far, but the same ideas apply to LogisticRegression.</p></li>
<li><p>The two main differences:</p>
<ol class="arabic simple">
<li><p>We use a sigmoid for predicting i.e. <span class="math notranslate nohighlight">\(\hat{y_i} = \sigma(\sum_j w_j x_{i j} + b)\)</span>
<br></p></li>
<li><p>We replace <span class="math notranslate nohighlight">\(|\hat{y_i} - y_i|^2\)</span> with a different equation appropriate for classification.</p></li>
</ol>
</li>
<li><p>By default scikit-learn uses an L2 penalty for LogisticRegression.</p>
<ul>
<li><p>You can change this using the <code class="docutils literal notranslate"><span class="pre">penalty</span></code> argument.</p></li>
<li><p>You may have to also change the <code class="docutils literal notranslate"><span class="pre">solver</span></code> argument.</p></li>
</ul>
</li>
</ul>
</section>
<section id="optional">
<h3>(Optional)<a class="headerlink" href="#optional" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_l2_pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">StandardScaler</span><span class="p">(),</span> 
    <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cross_validate</span><span class="p">(</span><span class="n">lr_l2_pipe</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fit_time       0.002410
score_time     0.000774
test_score     0.968233
train_score    0.987681
dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_l1_pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">StandardScaler</span><span class="p">(),</span>
    <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s2">&quot;l1&quot;</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s2">&quot;liblinear&quot;</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cross_validate</span><span class="p">(</span><span class="n">lr_l1_pipe</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fit_time       0.001834
score_time     0.000752
test_score     0.971805
train_score    0.986803
dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_l2_pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">lr_l1_pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">lr_l1_pipe</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;logisticregression&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mean radius                0.000000
mean texture              -0.170928
mean perimeter             0.000000
mean area                  0.000000
mean smoothness            0.000000
mean compactness           0.000000
mean concavity             0.000000
mean concave points       -0.441947
mean symmetry             -0.363758
mean fractal dimension     0.000000
radius error              -2.901975
texture error              0.000000
perimeter error            0.000000
area error                 0.000000
smoothness error           0.000000
compactness error          0.600830
concavity error            0.000000
concave points error       0.000000
symmetry error             0.000000
fractal dimension error    0.768830
worst radius              -2.234648
worst texture             -1.020808
worst perimeter            0.000000
worst area                 0.000000
worst smoothness           0.000000
worst compactness          0.000000
worst concavity           -1.253188
worst concave points      -2.353957
worst symmetry            -0.071447
worst fractal dimension    0.000000
dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;L1 logistic regression uses </span><span class="si">{}</span><span class="s2"> features&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lr_l1_pipe</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;logisticregression&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;L2 logistic regression uses </span><span class="si">{}</span><span class="s2"> features&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">lr_l2_pipe</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;logisticregression&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">coef_</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>L1 logistic regression uses 11 features
L2 logistic regression uses 30 features
</pre></div>
</div>
</div>
</div>
</section>
<section id="l1-vs-l2-penalty-summary-optional">
<h3>L1 vs L2 penalty summary (Optional)<a class="headerlink" href="#l1-vs-l2-penalty-summary-optional" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The L1 penalty will usually set some coefficients to zero</p></li>
<li><p>The L2 penalty will make coefficients smaller but rarely set them to zero</p></li>
<li><p>When correlated variables are present:</p>
<ul>
<li><p>L1 will usually keep one and set the other coefficients to zero</p></li>
<li><p>L2 will usually shrink the coefficient of all of them</p></li>
</ul>
</li>
<li><p>L1 and L2 penalties can be combined!</p>
<ul>
<li><p>This is called the <strong>elastic net</strong> penalty and is available in sklearn.</p></li>
</ul>
</li>
</ul>
</section>
<section id="model-based-selection">
<h3>Model-based selection<a class="headerlink" href="#model-based-selection" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Use a supervised machine learning model to judge the importance of each feature.</p></li>
<li><p>Keep only the most important once.</p></li>
<li><p>Supervised machine learning model used for feature selection can be different that the one used as the final estimator.</p></li>
<li><p>Use a model which has some way to calculate feature importances.</p></li>
</ul>
<ul class="simple">
<li><p>To use model-based selection, we use <code class="docutils literal notranslate"><span class="pre">SelectFromModel</span></code> transformer.</p></li>
<li><p>It selects features which have the feature importances greater than the provided threshold.</p></li>
<li><p>Below I’m using <code class="docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code> for feature selection with threshold “median” of feature importances.</p></li>
<li><p>Approximately how many features will be selected?</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">SelectFromModel</span>

<span class="n">select_rf</span> <span class="o">=</span> <span class="n">SelectFromModel</span><span class="p">(</span>
    <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span> 
    <span class="n">threshold</span><span class="o">=</span><span class="s2">&quot;median&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can put the feature selection transformer in a pipeline.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_lr_model_based</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">select_rf</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">cross_validate</span><span class="p">(</span><span class="n">pipe_lr_model_based</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fit_time       0.080629
score_time     0.005590
test_score     0.950564
train_score    0.974480
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Note that RandomForest is doing the feature selction and LogisticRegression is doing the classification with the reduced set of features.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_lr_model_based</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">pipe_lr_model_based</span><span class="o">.</span><span class="n">named_steps</span><span class="p">[</span><span class="s2">&quot;selectfrommodel&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(284, 15)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Similar results with only 15 features instead of 30 features.</p></li>
<li><p>Interestingly L1 regularization does better with fewer features here but that will vary on other data.</p></li>
</ul>
<p>Can we use KNN to select features?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">select_knn</span> <span class="o">=</span> <span class="n">SelectFromModel</span><span class="p">(</span>
    <span class="n">KNeighborsClassifier</span><span class="p">(),</span> 
    <span class="n">threshold</span><span class="o">=</span><span class="s2">&quot;median&quot;</span>
<span class="p">)</span>

<span class="n">pipe_lr_model_based</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">select_knn</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># This will raise an error!</span>
<span class="c1"># pd.DataFrame(</span>
<span class="c1">#    cross_validate(pipe_lr_model_based, X_train, y_train, return_train_score=True)#</span>
<span class="c1"># ).mean()</span>
</pre></div>
</div>
</div>
</div>
<p><strong>No</strong> KNN won’t work since it does not report feature importances.</p>
<p>What about SVC?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">select_svc</span> <span class="o">=</span> <span class="n">SelectFromModel</span><span class="p">(</span>
    <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">),</span> <span class="n">threshold</span><span class="o">=</span><span class="s2">&quot;median&quot;</span>
<span class="p">)</span>

<span class="n">pipe_lr_model_based</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">StandardScaler</span><span class="p">(),</span> <span class="n">select_svc</span><span class="p">,</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># This will raise an error with RBF kernel but linear works!</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
   <span class="n">cross_validate</span><span class="p">(</span><span class="n">pipe_lr_model_based</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fit_time       0.003196
score_time     0.000954
test_score     0.975313
train_score    0.987677
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>Only with a linear kernel but not with RBF kernel</p>
</section>
<section id="iclicker-exercise-14-2">
<h3>(iClicker) Exercise 14.2<a class="headerlink" href="#iclicker-exercise-14-2" title="Link to this heading">#</a></h3>
<p><strong>iClicker cloud join link: https://join.iclicker.com/HTRZ</strong></p>
<p><strong>Select all of the following statements which are TRUE.</strong></p>
<ul class="simple">
<li><p>(A) L2 regularized logistic regression sets correlated coefficients to zero *</p></li>
<li><p>(B) Lasso regression predicts values using <span class="math notranslate nohighlight">\(\hat{y_i} = \sum_j w_j x_{i j} + b\)</span> *</p></li>
<li><p>(C) Lasso and Ridge regression optimize different measures of “error” between predicted and observe target values *</p></li>
<li><p>(D) KNN and SVM RBF can be used with SelectFromModel</p></li>
<li><p>(E) I saw way to much math today! *</p></li>
</ul>
<p>* Denotes optional details only covered in our section</p>
</section>
<section id="recursive-feature-elimination-rfe">
<h3>Recursive feature elimination (RFE)<a class="headerlink" href="#recursive-feature-elimination-rfe" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Build a series of models</p></li>
<li><p>At each iteration, discard the least important feature according to the model.</p></li>
<li><p>Computationally expensive</p></li>
<li><p>Basic idea</p>
<ul>
<li><p>fit model</p></li>
<li><p>find least important feature</p></li>
<li><p>remove</p></li>
<li><p>iterate.</p></li>
</ul>
</li>
</ul>
</section>
<section id="rfe-algorithm">
<h3>RFE algorithm<a class="headerlink" href="#rfe-algorithm" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Decide <span class="math notranslate nohighlight">\(k\)</span>, the number of features to select.</p></li>
<li><p>Assign importances to features, e.g. by fitting a model and looking at <code class="docutils literal notranslate"><span class="pre">coef_</span></code> or <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code>.</p></li>
<li><p>Remove the least important feature.</p></li>
<li><p>Repeat steps 2-3 until only <span class="math notranslate nohighlight">\(k\)</span> features are remaining.</p></li>
</ol>
<p>Note that this is <strong>not</strong> the same as just removing all the less important features in one shot!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">RFE</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>

<span class="c1"># create ranking of features</span>
<span class="n">rfe</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(),</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">rfe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">rfe</span><span class="o">.</span><span class="n">ranking_</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([16, 12, 19, 13, 23, 20, 10,  1,  9, 22,  2, 25,  5,  7, 15,  4, 26,
       18, 21,  8,  1,  1,  1,  6, 14, 24,  3,  1, 17, 11])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">rfe</span><span class="o">.</span><span class="n">support_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[False False False False False False False  True False False False False
 False False False False False False False False  True  True  True False
 False False False  True False False]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;selected features: &quot;</span><span class="p">,</span> <span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">rfe</span><span class="o">.</span><span class="n">support_</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>selected features:  Index([&#39;mean concave points&#39;, &#39;worst radius&#39;, &#39;worst texture&#39;,
       &#39;worst perimeter&#39;, &#39;worst concave points&#39;],
      dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>How do we know what value to pass to <code class="docutils literal notranslate"><span class="pre">n_features_to_select</span></code>?</p></li>
</ul>
<ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">RFECV</span></code> which uses cross-validation to select number of features.</p></li>
</ul>
<p>For illustration purposes</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">RFECV</span>

<span class="n">rfe_cv</span> <span class="o">=</span> <span class="n">RFECV</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">),</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">rfe_cv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rfe_cv</span><span class="o">.</span><span class="n">support_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">rfe_cv</span><span class="o">.</span><span class="n">support_</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[False  True False  True False False  True  True  True False  True False
  True  True False  True False False False  True  True  True  True  True
 False False  True  True False  True]
Index([&#39;mean texture&#39;, &#39;mean area&#39;, &#39;mean concavity&#39;, &#39;mean concave points&#39;,
       &#39;mean symmetry&#39;, &#39;radius error&#39;, &#39;perimeter error&#39;, &#39;area error&#39;,
       &#39;compactness error&#39;, &#39;fractal dimension error&#39;, &#39;worst radius&#39;,
       &#39;worst texture&#39;, &#39;worst perimeter&#39;, &#39;worst area&#39;, &#39;worst concavity&#39;,
       &#39;worst concave points&#39;, &#39;worst fractal dimension&#39;],
      dtype=&#39;object&#39;)
</pre></div>
</div>
</div>
</div>
<p>But we should really use pipelines</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rfe_pipe</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">StandardScaler</span><span class="p">(),</span>
    <span class="n">RFECV</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">2000</span><span class="p">),</span> <span class="n">cv</span><span class="o">=</span><span class="mi">10</span><span class="p">),</span>
    <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
<span class="p">)</span>

<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cross_validate</span><span class="p">(</span><span class="n">rfe_pipe</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fit_time       0.452765
score_time     0.003247
test_score     0.943609
train_score    1.000000
dtype: float64
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Slow because there is cross validation within cross validation</p></li>
<li><p>Not a big improvement in scores compared to all features on this toy case</p></li>
</ul>
</section>
<section id="optional-search-and-score">
<h3>(Optional) Search and score<a class="headerlink" href="#optional-search-and-score" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Define a <strong>scoring function</strong> <span class="math notranslate nohighlight">\(f(S)\)</span> that measures the quality of the set of features <span class="math notranslate nohighlight">\(S\)</span>.</p></li>
<li><p>Now <strong>search</strong> for the set of features <span class="math notranslate nohighlight">\(S\)</span> with the best score.</p></li>
</ul>
</section>
<section id="general-idea-of-search-and-score-methods">
<h3>General idea of search and score methods<a class="headerlink" href="#general-idea-of-search-and-score-methods" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Example: Suppose you have three features: <span class="math notranslate nohighlight">\(A, B, C\)</span></p>
<ul>
<li><p>Compute <strong>score</strong> for <span class="math notranslate nohighlight">\(S = \{\}\)</span></p></li>
<li><p>Compute <strong>score</strong> for <span class="math notranslate nohighlight">\(S = \{A\}\)</span></p></li>
<li><p>Compute <strong>score</strong> for <span class="math notranslate nohighlight">\(S= \{B\}\)</span></p></li>
<li><p>Compute <strong>score</strong> for <span class="math notranslate nohighlight">\(S = \{C\}\)</span></p></li>
<li><p>Compute <strong>score</strong> for <span class="math notranslate nohighlight">\(S = \{A,B\}\)</span></p></li>
<li><p>Compute <strong>score</strong> for <span class="math notranslate nohighlight">\(S = \{A,C\}\)</span></p></li>
<li><p>Compute <strong>score</strong> for <span class="math notranslate nohighlight">\(S = \{B,C\}\)</span></p></li>
<li><p>Compute <strong>score</strong> for <span class="math notranslate nohighlight">\(S = \{A,B,C\}\)</span></p></li>
</ul>
</li>
<li><p>Return <span class="math notranslate nohighlight">\(S\)</span> with the best score.</p></li>
<li><p>How many distinct combinations do we have to try out?</p></li>
</ul>
</section>
<section id="optional-forward-or-backward-selection">
<h3>(Optional) Forward or backward selection<a class="headerlink" href="#optional-forward-or-backward-selection" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Also called wrapper methods</p></li>
<li><p>Shrink or grow feature set by removing or adding one feature at a time</p></li>
<li><p>Makes the decision based on whether adding/removing the feature improves the CV score or not</p></li>
</ul>
<p><img alt="" src="../../_images/forward_selection.png" /></p>
<!-- <img src='img/forward_selection.png' width="1000" height="1000" /> --><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.feature_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">SequentialFeatureSelector</span>

<span class="n">pipe_forward</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">StandardScaler</span><span class="p">(),</span>
    <span class="n">SequentialFeatureSelector</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span> 
                              <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;forward&quot;</span><span class="p">,</span> 
                              <span class="n">n_features_to_select</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">,</span> 
                              <span class="n">tol</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span>
    <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">cross_validate</span><span class="p">(</span><span class="n">pipe_forward</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fit_time       2.713733
score_time     0.003326
test_score     0.933020
train_score    1.000000
dtype: float64
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pipe_forward</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
    <span class="n">StandardScaler</span><span class="p">(),</span>
    <span class="n">SequentialFeatureSelector</span><span class="p">(</span>
        <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span> 
                           <span class="n">direction</span><span class="o">=</span><span class="s2">&quot;backward&quot;</span><span class="p">,</span> 
                           <span class="n">n_features_to_select</span><span class="o">=</span><span class="mi">15</span><span class="p">),</span>
    <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
    <span class="n">cross_validate</span><span class="p">(</span><span class="n">pipe_forward</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>fit_time       2.821208
score_time     0.003221
test_score     0.950627
train_score    1.000000
dtype: float64
</pre></div>
</div>
</div>
</div>
</section>
<section id="other-ways-to-search">
<h3>Other ways to search<a class="headerlink" href="#other-ways-to-search" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Stochastic local search</p>
<ul>
<li><p>Inject randomness so that we can explore new parts of the search space</p></li>
<li><p>Simulated annealing</p></li>
<li><p>Genetic algorithms</p></li>
</ul>
</li>
</ul>
</section>
<section id="iclicker-exercise-14-3">
<h3>(iClicker) Exercise 14.3<a class="headerlink" href="#iclicker-exercise-14-3" title="Link to this heading">#</a></h3>
<p><strong>iClicker cloud join link: https://join.iclicker.com/HTRZ</strong></p>
<p><strong>Select all of the following statements which are TRUE.</strong></p>
<ul class="simple">
<li><p>(A) Simple association-based feature selection approaches do not take into account the interaction between features.</p></li>
<li><p>(B) You can carry out feature selection using linear models by pruning the features which have very small weights (i.e., coefficients less than a threshold).</p></li>
<li><p>(C) The order of features removed given by <code class="docutils literal notranslate"><span class="pre">rfe.ranking_</span></code> is the same as the order of original feature importances given by the model.</p></li>
<li><p>(D) If you remove 10 features in a single step based on feature importance, the same 10 features would be removed if we performed sequential removal calculating feature importance after removing each feature.</p></li>
<li><p>(E) Forward search is guaranteed to find the best feature set. *</p></li>
</ul>
<p>* Denotes optional question</p>
</section>
</section>
<section id="optional-problems-with-feature-selection">
<h2>(Optional) Problems with feature selection<a class="headerlink" href="#optional-problems-with-feature-selection" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The term ‘relevance’ is not clearly defined.</p></li>
<li><p>What all things can go wrong with feature selection?</p></li>
<li><p>Attribution: From CPSC 340.</p></li>
</ul>
<section id="example-is-relevance-clearly-defined">
<h3>Example: Is “Relevance” clearly defined?<a class="headerlink" href="#example-is-relevance-clearly-defined" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Consider a supervised classification task of predicting whether someone has particular genetic variation (SNP)</p></li>
</ul>
<a class="reference internal image-reference" href="../../_images/sex_mom_dad.png"><img alt="../../_images/sex_mom_dad.png" src="../../_images/sex_mom_dad.png" style="width: 600px; height: 600px;" /></a>
<ul class="simple">
<li><p>True model: You almost have the same value as your biological mom.</p></li>
</ul>
</section>
<section id="is-relevance-clearly-defined">
<h3>Is “Relevance” clearly defined?<a class="headerlink" href="#is-relevance-clearly-defined" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>True model: You almost have the same value for SNP as your biological mom.</p>
<ul>
<li><p>(SNP = biological mom) with very high probability</p></li>
<li><p>(SNP != biological mom) with very low probability</p></li>
</ul>
</li>
</ul>
<a class="reference internal image-reference" href="../../_images/SNP.png"><img alt="../../_images/SNP.png" src="../../_images/SNP.png" style="width: 400px; height: 400px;" /></a>
<section id="id2">
<h4>Is “Relevance” clearly defined?<a class="headerlink" href="#id2" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>What if “mom” feature is repeated?</p></li>
<li><p>Should we pick both? Should we pick one of them because it predicts the other?</p></li>
<li><p>Dependence, collinearity for linear models</p>
<ul>
<li><p>If a feature can be predicted from the other, don’t know which one to pick.</p></li>
</ul>
</li>
</ul>
<a class="reference internal image-reference" href="../../_images/sex_mom_mom2_dad.png"><img alt="../../_images/sex_mom_mom2_dad.png" src="../../_images/sex_mom_mom2_dad.png" style="width: 600px; height: 600px;" /></a>
</section>
</section>
<section id="id3">
<h3>Is “Relevance” clearly defined?<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What if we add (maternal) “grandma” feature?</p></li>
<li><p>Is it relevant?</p>
<ul>
<li><p>We can predict SNP accurately using this feature</p></li>
</ul>
</li>
<li><p>Conditional independence</p>
<ul>
<li><p>But grandma is irrelevant given biological mom feature</p></li>
<li><p>Relevant features may become irrelevant given other features</p></li>
</ul>
</li>
</ul>
<a class="reference internal image-reference" href="../../_images/sex_mom_dad_grandma.png"><img alt="../../_images/sex_mom_dad_grandma.png" src="../../_images/sex_mom_dad_grandma.png" style="width: 600px; height: 600px;" /></a>
</section>
<section id="id4">
<h3>Is “Relevance” clearly defined?<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What if we do not know biological mom feature and we just have grandma feature</p></li>
<li><p>It becomes relevant now.</p>
<ul>
<li><p>Without mom feature this is the best we can do.</p></li>
</ul>
</li>
<li><p>General problem (“taco Tuesday” problem)</p>
<ul>
<li><p>Features can become relevant due to missing information</p></li>
</ul>
</li>
</ul>
<a class="reference internal image-reference" href="../../_images/sex_dad_grandma.png"><img alt="../../_images/sex_dad_grandma.png" src="../../_images/sex_dad_grandma.png" style="width: 600px; height: 600px;" /></a>
</section>
<section id="id5">
<h3>Is “Relevance” clearly defined?<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Are there any relevant features now?</p></li>
<li><p>They may have some common maternal ancestor.</p></li>
<li><p>What if mom likes dad because they share SNP?</p></li>
<li><p>General problem (Confounding)</p>
<ul>
<li><p>Hidden features can make irrelevant features relevant.</p></li>
</ul>
</li>
</ul>
<a class="reference internal image-reference" href="../../_images/sex_dad.png"><img alt="../../_images/sex_dad.png" src="../../_images/sex_dad.png" style="width: 600px; height: 600px;" /></a>
</section>
<section id="id6">
<h3>Is “Relevance” clearly defined?<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Now what if we have “sibling” feature?</p></li>
<li><p>The feature is relevant in predicting SNP but not the cause of SNP.</p></li>
<li><p>General problem (non causality)</p>
<ul>
<li><p>the relevant feature may not be causal</p></li>
</ul>
</li>
</ul>
<a class="reference internal image-reference" href="../../_images/sex_dad_sibling.png"><img alt="../../_images/sex_dad_sibling.png" src="../../_images/sex_dad_sibling.png" style="width: 600px; height: 600px;" /></a>
</section>
<section id="id7">
<h3>Is “Relevance” clearly defined?<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>What if you are given “baby” feature?</p></li>
<li><p>Now the sex feature becomes relevant.</p>
<ul>
<li><p>“baby” feature is relevant when sex == F</p></li>
</ul>
</li>
<li><p>General problem (context specific relevance)</p>
<ul>
<li><p>adding a feature can make an irrelevant feature relevant</p></li>
</ul>
</li>
</ul>
<a class="reference internal image-reference" href="../../_images/sex_dad_baby.png"><img alt="../../_images/sex_dad_baby.png" src="../../_images/sex_dad_baby.png" style="width: 600px; height: 600px;" /></a>
</section>
<section id="warnings-about-feature-selection">
<h3>Warnings about feature selection<a class="headerlink" href="#warnings-about-feature-selection" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>A feature is only relevant in the context of other features.</p>
<ul>
<li><p>Adding/removing features can make features relevant/irrelevant.</p></li>
</ul>
</li>
<li><p>Confounding factors can make irrelevant features the most relevant.</p></li>
<li><p>If features can be predicted from other other features, you cannot know which one to pick.</p></li>
<li><p>Relevance for features does not have a causal relationship.</p></li>
<li><p>Is feature selection completely hopeless?</p>
<ul>
<li><p>It is messy but we still need to do it. So we try to do our best!</p></li>
</ul>
</li>
</ul>
</section>
<section id="general-advice-on-finding-relevant-features">
<h3>General advice on finding relevant features<a class="headerlink" href="#general-advice-on-finding-relevant-features" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Try forward selection.</p></li>
<li><p>Try other feature selection methods (e.g., <code class="docutils literal notranslate"><span class="pre">RFE</span></code>, simulated annealing, genetic algorithms)</p></li>
<li><p>Try LASSO <strong>(Not part of course, but useful in real life)</strong></p></li>
<li><p>Talk to domain experts; they probably have an idea why certain features are relevant.</p></li>
<li><p>Don’t be overconfident.</p>
<ul>
<li><p>The methods we have seen probably do not discover the ground truth and how the world really works.</p></li>
<li><p>They simply tell you which features help in predicting <span class="math notranslate nohighlight">\(y_i\)</span>.</p></li>
</ul>
</li>
</ul>
<section id="relevant-resources">
<h4>Relevant resources<a class="headerlink" href="#relevant-resources" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Genome-wide_association_study">Genome-wide association study</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/feature_selection.html">sklearn feature selection</a></p></li>
<li><p><a class="reference external" href="https://www.youtube.com/watch?v=ioXKxulmwVQ">PyData: A Practical Guide to Dimensionality Reduction Techniques</a></p></li>
</ul>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "conda-env-cpsc330-py"
        },
        kernelOptions: {
            name: "conda-env-cpsc330-py",
            path: "./lectures/204-Andy-lectures"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'conda-env-cpsc330-py'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#announcements">Announcements</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-outcomes">Learning outcomes</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-engineering-motivation">Feature engineering: Motivation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iclicker-exercise-14-1">iClicker Exercise 14.1</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#garbage-in-garbage-out">Garbage in, garbage out.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-feature-engineering">What is feature engineering?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#some-quotes-on-feature-engineering">Some quotes on feature engineering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#better-features-usually-help-more-than-a-better-model">Better features usually help more than a better model.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-best-features-may-be-dependent-on-the-model-you-use">The best features may be dependent on the model you use.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#domain-specific-transformations">Domain-specific transformations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#domain-specific-transformations-genomics">Domain-specific transformations (Genomics)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Domain-specific transformations (Genomics)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-interactions-and-feature-crosses">Feature interactions and feature crosses</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-crosses-for-one-hot-encoded-features">Feature crosses for one-hot encoded features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#demo-of-feature-engineering-with-numeric-features">Demo of feature engineering with numeric features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interim-summary">Interim summary</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-engineering">Feature engineering</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-selection-introduction-and-motivation">Feature selection: Introduction and motivation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-feature-selection">What is feature selection?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-feature-selection">Why feature selection?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-carry-out-feature-selection">How do we carry out feature selection?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-models-recap-optional">Linear models recap (Optional)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression-l2-penalty-optional">Ridge regression (L2 penalty) (Optional)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression-l1-penalty-optional">Lasso regression (L1 penalty) (Optional)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-l1-and-l2-optional">Logistic regression (L1 and L2) (Optional)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional">(Optional)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l1-vs-l2-penalty-summary-optional">L1 vs L2 penalty summary (Optional)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-based-selection">Model-based selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iclicker-exercise-14-2">(iClicker) Exercise 14.2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recursive-feature-elimination-rfe">Recursive feature elimination (RFE)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rfe-algorithm">RFE algorithm</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-search-and-score">(Optional) Search and score</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-idea-of-search-and-score-methods">General idea of search and score methods</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-forward-or-backward-selection">(Optional) Forward or backward selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-ways-to-search">Other ways to search</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#iclicker-exercise-14-3">(iClicker) Exercise 14.3</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optional-problems-with-feature-selection">(Optional) Problems with feature selection</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-is-relevance-clearly-defined">Example: Is “Relevance” clearly defined?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#is-relevance-clearly-defined">Is “Relevance” clearly defined?</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Is “Relevance” clearly defined?</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Is “Relevance” clearly defined?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Is “Relevance” clearly defined?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Is “Relevance” clearly defined?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Is “Relevance” clearly defined?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Is “Relevance” clearly defined?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#warnings-about-feature-selection">Warnings about feature selection</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#general-advice-on-finding-relevant-features">General advice on finding relevant features</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#relevant-resources">Relevant resources</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Varada Kolhatkar
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>